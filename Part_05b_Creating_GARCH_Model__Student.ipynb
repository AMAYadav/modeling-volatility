{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fintechsteve/modeling-volatility/blob/master/Part_04b_Creating_Turbulence_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BtCSSGGdu4Qt"
   },
   "source": [
    "## Part 05 (b): Creating a GARCH Model\n",
    "\n",
    "### In this section you will:\n",
    "\n",
    "\n",
    "*   Load volatility forecasts with a rolling window.\n",
    "*   Transform the forecasts into set of dynamic weights and track performance of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ETMoGdnNrtjj"
   },
   "source": [
    "## Import all necessary libraries\n",
    "\n",
    "For this piece, we will need the following packages to be available to our environment:\n",
    "\n",
    "*   Numpy and Pandas (For data manipulation)\n",
    "*   DateTime (For basic date manipulation)\n",
    "*   Matplotlib (For timeseries vizualization)\n",
    "\n",
    "If the packages are not available, install the with \"pip install X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FS6GYKq6T-ih"
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import cloudpickle as cp\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import matplotlib.pyplot as plt\n",
    "import bisect\n",
    "from scipy import stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./returns.pkl', 'rb') as f:\n",
    "  returns = pickle.load(f)\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBoMCDKYsdli"
   },
   "source": [
    "### Read in data from previously stored vol_pred.pkl file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vol_pred = cp.load(urllib.request.urlopen('https://github.com/fintechsteve/modeling-volatility/blob/master/vol_pred.pkl?raw=true'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you have estimated the models locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "cFySsoztT-i9",
    "outputId": "b9d576fc-9915-4c7e-cba7-1daa1e965846"
   },
   "outputs": [],
   "source": [
    "with open('./vol_pred.pkl', 'rb') as f:\n",
    "    df_vol_pred = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vol_pred.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot p = 2, q = 3 as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_pred_23 = df_vol_pred[(df_vol_pred['p']==2) & (df_vol_pred['q']==3)][['date', 'var']]\n",
    "vol_pred_23 = vol_pred_23.set_index('date')\n",
    "vol_pred_23.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrZTZ568uGXu"
   },
   "source": [
    "### Convert rolling volatility forecast into a weight \n",
    "\n",
    "We are going to use linear ranking to create weights, just as we did for the Turbulence signal.\n",
    "\n",
    "The linear ranking, when used in-sample, should map results to a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "lCW0NX0odY_R",
    "outputId": "b1aba278-ea8c-4055-8bf8-71bb35044119"
   },
   "outputs": [],
   "source": [
    "vol_pred_23.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "zqf9ivy6XYe4",
    "outputId": "2f1707ba-2cb5-43a0-9576-501a89d6c6cb"
   },
   "outputs": [],
   "source": [
    "vol_pred_23_rank = vol_pred_23.dropna().rank()/vol_pred_23.count()\n",
    "vol_pred_23_rank.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "X4bJWUpHE1pR",
    "outputId": "9f608dfe-4c59-49ad-c2a8-bdc9d10300f5"
   },
   "outputs": [],
   "source": [
    "insample_start_date = '1975-12-30'\n",
    "insample_end_date = '2000-01-01'\n",
    "insample_vol_pred_23 = (vol_pred_23.loc[insample_start_date:insample_end_date]).dropna().values\n",
    "insample_vol_pred_23.sort()\n",
    "num_insample = np.size(insample_vol_pred_23)\n",
    "vol_pred_23_ranked = 1-np.true_divide(list(map(lambda x:float('NaN') if np.isnan(x) else bisect.bisect_left(insample_vol_pred_23,x), vol_pred_23['var'])),num_insample)\n",
    "plt.plot(vol_pred_23_ranked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5GYgPevt22DJ"
   },
   "outputs": [],
   "source": [
    "def garch_weights(vol, insample_start_date, insample_end_date):\n",
    "  insample_vol = (vol.loc[insample_start_date:insample_end_date]).dropna().values\n",
    "  insample_vol.sort()\n",
    "  num_insample = np.size(insample_vol)\n",
    "  vol_weights = pd.DataFrame(1-np.true_divide(list(map(lambda x:float('NaN') if np.isnan(x) else bisect.bisect_left(insample_vol,x), vol['var'])),num_insample), index=vol.index)\n",
    "  vol_weights.columns=['weights']\n",
    "  return vol_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "colab_type": "code",
    "id": "MjKhquX373So",
    "outputId": "34da9642-1249-470c-9307-d7a017d2da9a"
   },
   "outputs": [],
   "source": [
    "vol_weights = garch_weights(vol_pred_23, '1975-12-30', '2000-01-01')\n",
    "vol_weights.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick sanity checks\n",
    "\n",
    "1) Are the weights aligned with decreasing volatility?\n",
    "2) Does the weight inversely correlate with subsequent volatility?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([vol_pred_23, vol_weights],axis=1).plot(kind='scatter', x='weights', y='var',\n",
    "                                           color='Red', label='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxy_weight = [0, 0.119, 0.036, 0, 0.136, 0.576, 0, 0, 0.091]\n",
    "dxy = pd.DataFrame(returns.dot(dxy_weight))\n",
    "dxy_trunc = dxy[insample_start_date:'2017-12-22'] ##the last few days are missing?, I need to redo the GARCH stuff to get the last couple days\n",
    "dxy_forward = dxy_trunc.shift(-1)\n",
    "dxy_forward_sq = (dxy_forward**2).rename(columns={0:'dxyfsq'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highvol = vol_weights['weights']<.25\n",
    "pd.DataFrame([[np.sqrt(261)*dxy_trunc[highvol].std(), np.sqrt(261)*dxy_trunc[~highvol].std()], [sum(highvol), sum(~highvol)]], index=['std','N'], columns=['High Vol','Low Vol'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N20M0VTMyxil"
   },
   "source": [
    "### Calculate the performance of the GARCH model\n",
    "\n",
    "Leveraging the framework we developed in Part 3, we calculate the excess performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "naWuvBdzu-NZ"
   },
   "outputs": [],
   "source": [
    "def model_perf(model_wts, dxyfsq, framework_params):\n",
    "  # Note, since chapter 3, modified the code a little to handle the possibility that column headings are not constant\n",
    "  # Also added an estimation window, which we will use later\n",
    "    \n",
    "  model_vol = pd.DataFrame()\n",
    "  model_vol['total'] = model_wts[model_wts.columns[0]]*dxyfsq[dxyfsq.columns[0]] \n",
    "  model_vol['avg'] = (np.mean(model_wts.loc[framework_params['insample_start_date']:framework_params['insample_end_date']].dropna()[model_wts.columns[0]])*dxyfsq)\n",
    "\n",
    "  is_model_vol = model_vol.loc[framework_params['insample_start_date']:framework_params['insample_end_date']].dropna() \n",
    "  is_vol_rmse = np.sqrt(np.mean(is_model_vol))\n",
    "\n",
    "  out_model_vol = model_vol.loc[framework_params['outofsample_start_date']:framework_params['outofsample_end_date']].dropna()\n",
    "  out_vol_rmse = np.sqrt(np.mean(out_model_vol))\n",
    "  \n",
    "  model_perf_stats = {'insample rmse': is_vol_rmse['total'], 'insample excess rmse': is_vol_rmse['total'] - is_vol_rmse['avg'],\n",
    "                        'out-of-sample rmse': out_vol_rmse['total'], 'out-of-sample excess rmse': out_vol_rmse['total'] - out_vol_rmse['avg']}\n",
    "    \n",
    "  if framework_params.get('estimationsample_start_date'):\n",
    "    est_model_vol = model_vol.loc[framework_params['estimationsample_start_date']:framework_params['estimationsample_end_date']].dropna()\n",
    "    est_vol_rmse = np.sqrt(np.mean(est_model_vol))\n",
    "    model_perf_stats['estimation rmse'] = est_vol_rmse['total']\n",
    "    model_perf_stats['estimation excess rmse'] = est_vol_rmse['total'] - est_vol_rmse['avg']\n",
    "        \n",
    "  if framework_params.get('verbose',False):\n",
    "    model_perf_stats['insample model volatility'] = is_model_vol\n",
    "    model_perf_stats['out-of-sample model volatility'] = out_model_vol\n",
    "    \n",
    "  return model_perf_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "VL5tpBzRSQJp",
    "outputId": "ec6dbff2-a4ad-4be2-d229-a9db27c035ec"
   },
   "outputs": [],
   "source": [
    "framework_params = {'insample_start_date': '1975-12-30',\n",
    "                    'insample_end_date': '2000-01-01',\n",
    "                    'outofsample_start_date': '2000-01-02',\n",
    "                    'outofsample_end_date': '2017-12-22',\n",
    "                    'verbose': False}\n",
    "\n",
    "model_perf_stats = model_perf(vol_weights, dxy_forward_sq, framework_params)\n",
    "model_perf_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byOlnZO40EDx"
   },
   "source": [
    "### Test significance of excess volatility\n",
    "\n",
    "A simple test of the ability of a turbulence model to outperform a random model is to calculate the significance of the average excess volatility.\n",
    "\n",
    "We want this statistic to be negative, because a good model should reduce volatility.\n",
    "\n",
    "We can calculate this by looking at the difference between the volatility for the model and the volatility of a model that assigns an average weight to volatility and calculating a t-statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "stRMHt7qy6Pr",
    "outputId": "be6dd68a-cf72-4125-914d-dd6b991054c9"
   },
   "outputs": [],
   "source": [
    "framework_params = {'insample_start_date': '1975-01-02',\n",
    "                    'insample_end_date': '2000-01-01',\n",
    "                    'outofsample_start_date': '2000-01-02',\n",
    "                    'outofsample_end_date': '2017-12-22',\n",
    "                    'verbose': True}\n",
    "\n",
    "model_perf_stats = model_perf(vol_weights, dxy_forward_sq, framework_params)\n",
    "\n",
    "stats.ttest_rel(model_perf_stats['insample model volatility']['total'], model_perf_stats['insample model volatility']['avg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-93ZyVJIPwv6",
    "outputId": "921f3d48-2a8f-442c-85eb-263dcc606d9a"
   },
   "outputs": [],
   "source": [
    "stats.ttest_rel(model_perf_stats['out-of-sample model volatility']['total'], model_perf_stats['out-of-sample model volatility']['avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for the optimal parameters\n",
    "\n",
    "We have been working with the GARCH model with p = 2 and q = 3. How do we pick the correct parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the sample for hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_params = {'insample_start_date': '1975-12-30',\n",
    "                    'insample_end_date': '1994-12-31',\n",
    "                    'estimationsample_start_date': '1995-01-01',\n",
    "                    'estimationsample_end_date': '2004-12-31',\n",
    "                    'outofsample_start_date': '2005-01-01',\n",
    "                    'outofsample_end_date': '2017-12-22',\n",
    "                    'verbose': False}\n",
    "\n",
    "def garch_models_performance(vol_signal, framework_params):\n",
    "  vol_wts = garch_weights(vol_signal, framework_params['insample_start_date'], framework_params['insample_end_date'])\n",
    "  model_perf_stats = model_perf(vol_wts, dxy_forward_sq, framework_params)\n",
    "  return {'model_perf_stats':model_perf_stats, 'vol_wts':vol_wts}\n",
    "\n",
    "some_model_perf_stats = garch_models_performance(vol_pred_23, framework_params)\n",
    "some_model_perf_stats['model_perf_stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run for p=1, 2, and 3 and q = 1, 2, and 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ps = [1, 2, 3]\n",
    "qs = [1, 2, 3]\n",
    "\n",
    "index = pd.MultiIndex.from_product([ps,qs],names=['p','q'])\n",
    "\n",
    "garch_model_perf_stats = pd.DataFrame(index = index, columns = some_model_perf_stats['model_perf_stats'].keys())\n",
    "garch_model_weights = pd.DataFrame(index = returns.index, columns = index)\n",
    "\n",
    "for p in ps:\n",
    "  for q in qs:\n",
    "    vol_pred = df_vol_pred[(df_vol_pred['p']==p) & (df_vol_pred['q']==q)][['date', 'var']]\n",
    "    vol_pred = vol_pred.set_index('date')\n",
    "    model_output = garch_models_performance(vol_pred, framework_params)\n",
    "    garch_model_perf_stats.loc[(p,q)] = model_output['model_perf_stats']\n",
    "    garch_model_weights.loc[:,(p,q)] = model_output['vol_wts']['weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_model_perf_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_model_weights.to_pickle('./garch_model_weights.pkl')\n",
    "garch_model_perf_stats.to_pickle('./garch_model_perf_stats.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of sample performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the out of sample performance of the optimal model\n",
    "We plan on taking the model with the lowest estimation period excess RMSE as the optimal parameterization.\n",
    "\n",
    "Let's look at how well this aligns during the out of sample period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "garch_model_perf_stats.plot(x='estimation excess rmse', y='out-of-sample excess rmse', ax=ax, style='o')\n",
    "\n",
    "for k, v in garch_model_perf_stats.iterrows():\n",
    "  ax.annotate(k, (v['estimation excess rmse'],v['out-of-sample excess rmse']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, plotting the insample and out of sample excess RMSE to explore the overfitting problem:\n",
    "\n",
    "Hint: It's worse for GARCH..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "garch_model_perf_stats.plot(x='insample excess rmse', y='out-of-sample excess rmse', ax=ax, style='o')\n",
    "\n",
    "for k, v in garch_model_perf_stats.iterrows():\n",
    "  ax.annotate(k, (v['insample excess rmse'],v['out-of-sample excess rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Part_04b_Creating_Turbulence_Model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:modeliing-volatility] *",
   "language": "python",
   "name": "conda-env-modeliing-volatility-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
