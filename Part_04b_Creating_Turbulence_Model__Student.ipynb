{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fintechsteve/modeling-volatility/blob/master/Part_04b_Creating_Turbulence_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BtCSSGGdu4Qt"
   },
   "source": [
    "## Part 04 (b): Creating a Turbulence Model\n",
    "\n",
    "### In this section you will:\n",
    "\n",
    "\n",
    "*   Create a turbulence function that estimates turbulence with a rolling window.\n",
    "*   Transform a turbulence signal into set of dynamic weights. Track performance of the model.\n",
    "*   Optimize the lookback and smoothing window.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ETMoGdnNrtjj"
   },
   "source": [
    "## Import all necessary libraries\n",
    "\n",
    "For this piece, we will need the following packages to be available to our environment:\n",
    "\n",
    "*   Numpy and Pandas (For data manipulation)\n",
    "*   DateTime (For basic date manipulation)\n",
    "*   Matplotlib (For timeseries vizualization)\n",
    "\n",
    "If the packages are not available, install the with \"pip install X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FS6GYKq6T-ih"
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import bisect\n",
    "from scipy import stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBoMCDKYsdli"
   },
   "source": [
    "### Read in data from previously stored returns.pkl file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "cFySsoztT-i9",
    "outputId": "b9d576fc-9915-4c7e-cba7-1daa1e965846"
   },
   "outputs": [],
   "source": [
    "with open('./returns.pkl', 'rb') as f:\n",
    "    returns = pickle.load(f)\n",
    "    f.close()\n",
    "returns.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPfZk0NLtIaU"
   },
   "source": [
    "### Create a function that calculates turbulence for the whole window supplied using the last row as the observed return\n",
    "\n",
    "Note that for faster computation we can replace ``` np.linalg.pinv(covmat)@(r - mu) ``` with ``` np.linalg.lstsq(covmat, r - mu)[0] ```\n",
    "\n",
    "Also note that a more proper calculation of turbulence would only use rows 1:n-1 to calculate mean and covariance. But everybody seems to include n, so we aren't going to get upset here because it is simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyFwsRh7T-jC"
   },
   "outputs": [],
   "source": [
    "def simple_turbulence(rets):\n",
    "  meanret = rets.mean()\n",
    "  covrets = rets.cov()\n",
    "  turbvalue = pd.DataFrame((rets[-1:]-meanret)@np.linalg.lstsq(covrets,(rets[-1:]-meanret).T, rcond=None)[0],index=[rets.index[-1]])\n",
    "  return turbvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "qinK625FmwO3",
    "outputId": "6afc9696-e2a0-4e7e-f408-d5a22046bef2"
   },
   "outputs": [],
   "source": [
    "%time\n",
    "turb_singleday = simple_turbulence(returns)\n",
    "turb_singleday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rL7V05_kAzg"
   },
   "source": [
    "### Create a function that calculates rolling turbulence given a lookback window and a smoothing window\n",
    "\n",
    "To do this, we are going to create a function ```roll``` that generates a rolling window of returns for a specified window length, and a function that applies the turbulence calculation to that window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZYRisZw66GDy"
   },
   "outputs": [],
   "source": [
    "def roll(df, w):\n",
    "    for i in range(df.shape[0] - w + 1):\n",
    "        yield pd.DataFrame(df.values[i:i+w, :], df.index[i:i+w], df.columns)\n",
    "  \n",
    "def rolling_turbulence(rets, lookback=260, smoothing=20):\n",
    "    roll_turb = pd.concat([simple_turbulence(retwindow) for retwindow in roll(rets, lookback)], axis=0)\n",
    "    roll_turb.columns = ['daily']\n",
    "    roll_turb_smooth = pd.DataFrame(roll_turb.rolling(smoothing).mean()).rename(columns={'daily':'smoothed'})\n",
    "    rets = rets.join(roll_turb)\n",
    "    rets = rets.join(roll_turb_smooth)\n",
    "    return rets[['daily','smoothed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "colab_type": "code",
    "id": "al1TiJa-msmW",
    "outputId": "972c71e9-2991-4876-a9ed-c93885015b9e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "roll_turb = rolling_turbulence(returns, lookback=260, smoothing=20)\n",
    "roll_turb.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "lX2SH9yqI1e7",
    "outputId": "1e0bdcfe-bc87-4256-d19d-8453e1fe75e4"
   },
   "outputs": [],
   "source": [
    "roll_turb.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrZTZ568uGXu"
   },
   "source": [
    "### Convert rolling turbulence into a weight (And do so in a function)\n",
    "\n",
    "Turbulence as a raw statistic ranges between 0 and infinity. It cannot be directly used for investment strategies without some kind of transformation into model weights.\n",
    "\n",
    "Our most obvious optiions are to create a binary cutoff or linear ranking. We are going to use linear ranking, though there are reasons why you might want to use a binary cutoff (simple on/off model).\n",
    "\n",
    "The linear ranking, when used in-sample, should map results to a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "lCW0NX0odY_R",
    "outputId": "b1aba278-ea8c-4055-8bf8-71bb35044119"
   },
   "outputs": [],
   "source": [
    "roll_turb['smoothed'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "zqf9ivy6XYe4",
    "outputId": "2f1707ba-2cb5-43a0-9576-501a89d6c6cb"
   },
   "outputs": [],
   "source": [
    "historical_turb_rank = roll_turb['smoothed'].dropna().rank()/roll_turb['smoothed'].count()\n",
    "historical_turb_rank.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "X4bJWUpHE1pR",
    "outputId": "9f608dfe-4c59-49ad-c2a8-bdc9d10300f5"
   },
   "outputs": [],
   "source": [
    "insample_start_date = '1975-01-02'\n",
    "insample_end_date = '2000-01-01'\n",
    "insample_turb = (roll_turb['smoothed'].loc[insample_start_date:insample_end_date]).dropna().values\n",
    "insample_turb.sort()\n",
    "num_insample = np.size(insample_turb)\n",
    "roll_turb['ranked'] = 1-np.true_divide(list(map(lambda x:float('NaN') if np.isnan(x) else bisect.bisect_left(insample_turb,x), roll_turb['smoothed'])),num_insample)\n",
    "roll_turb['ranked'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5GYgPevt22DJ"
   },
   "outputs": [],
   "source": [
    "def turbulence_weights(roll_turb, insample_start_date, insample_end_date):\n",
    "  insample_turb = (roll_turb['smoothed'].loc[insample_start_date:insample_end_date]).dropna().values\n",
    "  insample_turb.sort()\n",
    "  num_insample = np.size(insample_turb)\n",
    "  turb_weights = pd.DataFrame(1-np.true_divide(list(map(lambda x:float('NaN') if np.isnan(x) else bisect.bisect_left(insample_turb,x), roll_turb['smoothed'])),num_insample), index=roll_turb.index)\n",
    "  turb_weights.columns=['weights']\n",
    "  return turb_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "colab_type": "code",
    "id": "MjKhquX373So",
    "outputId": "34da9642-1249-470c-9307-d7a017d2da9a"
   },
   "outputs": [],
   "source": [
    "turb_weights = turbulence_weights(roll_turb, '1975-01-02', '2000-01-01')\n",
    "turb_weights.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick sanity checks\n",
    "\n",
    "1) Are the weights aligned with decreasing turbulence?\n",
    "2) Does the weight inversely correlate with subsequent volatility?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([roll_turb, turb_weights],axis=1).plot(kind='scatter', x='weights', y='smoothed',\n",
    "                                           color='Red', label='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxy_weight = [0, 0.119, 0.036, 0, 0.136, 0.576, 0, 0, 0.091]\n",
    "dxy = pd.DataFrame(returns.dot(dxy_weight))\n",
    "dxy_forward = dxy.shift(-1)\n",
    "dxy_forward_sq = (dxy_forward**2).rename(columns={0:'dxyfsq'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highturbulence = turb_weights['weights']<.25\n",
    "pd.DataFrame([[np.sqrt(261)*dxy[highturbulence].std(), np.sqrt(261)*dxy[~highturbulence].std()], [sum(highturbulence), sum(~highturbulence)]], index=['std','N'], columns=['High Turb','Low Turb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N20M0VTMyxil"
   },
   "source": [
    "### Calculate the performance of the turbulence model\n",
    "\n",
    "Leveraging the framework we developed in Part 3, we calculate the excess performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "naWuvBdzu-NZ"
   },
   "outputs": [],
   "source": [
    "def model_perf(model_wts, dxyfsq, framework_params):\n",
    "  # Note, since chapter 3, modified the code a little to handle the possibility that column headings are not constant\n",
    "  # Also added an estimation window, which we will use later\n",
    "    \n",
    "  model_vol = pd.DataFrame()\n",
    "  model_vol['total'] = model_wts[model_wts.columns[0]]*dxyfsq[dxyfsq.columns[0]] \n",
    "  model_vol['avg'] = (np.mean(model_wts.loc[framework_params['insample_start_date']:framework_params['insample_end_date']].dropna()[model_wts.columns[0]])*dxyfsq)\n",
    "\n",
    "  is_model_vol = model_vol.loc[framework_params['insample_start_date']:framework_params['insample_end_date']].dropna() \n",
    "  is_vol_rmse = np.sqrt(np.mean(is_model_vol))\n",
    "\n",
    "  out_model_vol = model_vol.loc[framework_params['outofsample_start_date']:framework_params['outofsample_end_date']].dropna()\n",
    "  out_vol_rmse = np.sqrt(np.mean(out_model_vol))\n",
    "  \n",
    "  model_perf_stats = {'insample rmse': is_vol_rmse['total'], 'insample excess rmse': is_vol_rmse['total'] - is_vol_rmse['avg'],\n",
    "                        'out-of-sample rmse': out_vol_rmse['total'], 'out-of-sample excess rmse': out_vol_rmse['total'] - out_vol_rmse['avg']}\n",
    "    \n",
    "  if framework_params.get('estimationsample_start_date'):\n",
    "    est_model_vol = model_vol.loc[framework_params['estimationsample_start_date']:framework_params['estimationsample_end_date']].dropna()\n",
    "    est_vol_rmse = np.sqrt(np.mean(est_model_vol))\n",
    "    model_perf_stats['estimation rmse'] = est_vol_rmse['total']\n",
    "    model_perf_stats['estimation excess rmse'] = est_vol_rmse['total'] - est_vol_rmse['avg']\n",
    "        \n",
    "  if framework_params.get('verbose',False):\n",
    "    model_perf_stats['insample model volatility'] = is_model_vol\n",
    "    model_perf_stats['out-of-sample model volatility'] = out_model_vol\n",
    "    \n",
    "  return model_perf_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "VL5tpBzRSQJp",
    "outputId": "ec6dbff2-a4ad-4be2-d229-a9db27c035ec"
   },
   "outputs": [],
   "source": [
    "framework_params = {'insample_start_date': '1975-01-02',\n",
    "                    'insample_end_date': '2000-01-01',\n",
    "                    'outofsample_start_date': '2000-01-02',\n",
    "                    'outofsample_end_date': '2017-12-26',\n",
    "                    'verbose': False}\n",
    "\n",
    "model_perf(turb_weights, dxy_forward_sq, framework_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byOlnZO40EDx"
   },
   "source": [
    "## Test significance of excess volatility\n",
    "\n",
    "A simple test of the ability of a turbulence model to outperform a random model is to calculate the significance of the average excess volatility.\n",
    "\n",
    "We want this statistic to be negative, because a good model should reduce volatility.\n",
    "\n",
    "We can calculate this by looking at the difference between the volatility for the model and the volatility of a model that assigns an average weight to volatility and calculating a t-statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "stRMHt7qy6Pr",
    "outputId": "be6dd68a-cf72-4125-914d-dd6b991054c9"
   },
   "outputs": [],
   "source": [
    "framework_params = {'insample_start_date': '1975-01-02',\n",
    "                    'insample_end_date': '2000-01-01',\n",
    "                    'outofsample_start_date': '2000-01-02',\n",
    "                    'outofsample_end_date': '2017-12-26',\n",
    "                    'verbose': True}\n",
    "\n",
    "model_perf_stats = model_perf(turb_weights, dxy_forward_sq, framework_params)\n",
    "\n",
    "stats.ttest_rel(model_perf_stats['insample model volatility']['total'], model_perf_stats['insample model volatility']['avg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-93ZyVJIPwv6",
    "outputId": "921f3d48-2a8f-442c-85eb-263dcc606d9a"
   },
   "outputs": [],
   "source": [
    "stats.ttest_rel(model_perf_stats['out-of-sample model volatility']['total'], model_perf_stats['out-of-sample model volatility']['avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for the optimal parameters\n",
    "\n",
    "We chose a smoothing window of 20 days and a lookback of 260 days. How do we pick the correct parameters?\n",
    "\n",
    "We need to be careful about holdback periods with this kind of analysis. Let's start by being careful with our sample windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the sample for hyperparameter tuning\n",
    "\n",
    "We will limit ourselves to the following sample windows:\n",
    "\n",
    "insample_start_date: '1975-01-02'\n",
    "insample_end_date: '1994-12-31'\n",
    "estimationsample_start_date: '1995-01-01'\n",
    "estimationsample_end_date: '2004-12-31'\n",
    "outofsample_start_date: '2005-01-01'\n",
    "outofsample_end_date: '2017-12-26'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_params = {'insample_start_date': '1975-01-02',\n",
    "                    'insample_end_date': '1994-12-31',\n",
    "                    'estimationsample_start_date': '1995-01-01',\n",
    "                    'estimationsample_end_date': '2004-12-31',\n",
    "                    'outofsample_start_date': '2005-01-01',\n",
    "                    'outofsample_end_date': '2017-12-26',\n",
    "                    'verbose': False}\n",
    "\n",
    "def turbulence_models_performance(rets, framework_params, lookback, smoothing):\n",
    "  roll_turb_signal = rolling_turbulence(rets, lookback=lookback, smoothing=smoothing)\n",
    "  turb_wts = turbulence_weights(roll_turb_signal, framework_params['insample_start_date'], framework_params['insample_end_date'])\n",
    "  model_perf_stats = model_perf(turb_wts, dxy_forward_sq, framework_params)\n",
    "  return {'model_perf_stats':model_perf_stats, 'turb_wts':turb_wts}\n",
    "\n",
    "some_model_perf_stats = turbulence_models_performance(returns, framework_params, 260, 20)\n",
    "some_model_perf_stats['model_perf_stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run for the following lookback windows:\n",
    "65 days (1 Quarter), 130 days (Half year), 260 days (1 year), 390 days (1.5 years), 520 days (2 years)\n",
    "\n",
    "and the following smoothing windows:\n",
    "1 day (daily), 5 days (weekly), 10 days (fortnightly), 20 days (monthishly), 40 days (bimonthly)\n",
    "\n",
    "**Note this takes a little time to run for all 25 models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lookbacks = [65,130,260,390,520]\n",
    "smoothings = [1, 5, 10, 20, 40]\n",
    "\n",
    "index = pd.MultiIndex.from_product([lookbacks,smoothings],names=['lookbacks','smoothings'])\n",
    "\n",
    "turb_model_perf_stats = pd.DataFrame(index = index, columns = some_model_perf_stats['model_perf_stats'].keys())\n",
    "turb_model_weights = pd.DataFrame(index = returns.index, columns = index)\n",
    "\n",
    "for lookback in lookbacks:\n",
    "  for smoothing in smoothings:\n",
    "    model_output = turbulence_models_performance(returns, framework_params, lookback=lookback, smoothing=smoothing)\n",
    "    turb_model_perf_stats.loc[(lookback,smoothing)] = model_output['model_perf_stats']\n",
    "    turb_model_weights.loc[:,(lookback,smoothing)] = model_output['turb_wts']['weights']\n",
    "\n",
    "turb_model_perf_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_model_weights.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_model_weights.to_pickle('./turb_model_weights.pkl')\n",
    "turb_model_perf_stats.to_pickle('./turb_model_perf_stats.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the out of sample performance of the optimal model\n",
    "\n",
    "We plan on taking the model with the lowest estimation period excess RMSE as the optimal parameterization.\n",
    "\n",
    "Let's look at how well this aligns during the out of sample period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "turb_model_perf_stats.plot(x='estimation excess rmse', y='out-of-sample excess rmse', ax=ax, style='o')\n",
    "\n",
    "for k, v in turb_model_perf_stats.iterrows():\n",
    "  ax.annotate(k, (v['estimation excess rmse'],v['out-of-sample excess rmse']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a strong (but by no means 1:1) relationship between performance in the estimation window and out of sample, suggesting that these models exhibit persistent performance associated with paramater choices.\n",
    "\n",
    "### Exploring a bad idea in a little more detail...\n",
    "\n",
    "Suppose instead we had used the insample performance to choose the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "turb_model_perf_stats.plot(x='insample excess rmse', y='out-of-sample excess rmse', ax=ax, style='o')\n",
    "\n",
    "for k, v in turb_model_perf_stats.iterrows():\n",
    "  ax.annotate(k, (v['insample excess rmse'],v['out-of-sample excess rmse']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is a positive relationship between the two, the relationship is weaker. This is because we are overfitting in sample.\n",
    "\n",
    "**In sample performance should never be used to determine the correct parameterization of a model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ns85W-Cf5CT4"
   },
   "source": [
    "### Sanity checking with random returns\n",
    "\n",
    "Before you get too far with any model, it is worthwhile testing how your model performs if the inputs are randomized. This is a good way of identifying information leakage inside the model code.\n",
    "\n",
    "To do this with turbulence, we are going to simulate 1000 random samples of returns, calculate turbulence on these simulated returns and look at the relationship to subsequent simulated DXY. If we use IID normal returns, there should be no predictive power out of sample.\n",
    "\n",
    "** Note: This step takes a while to run, depending on computational power **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zcaj07aCaX6o"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "framework_params = {'insample_start_date': '1975-01-02',\n",
    "                    'insample_end_date': '2000-01-01',\n",
    "                    'outofsample_start_date': '2000-01-02',\n",
    "                    'outofsample_end_date': '2017-12-26',\n",
    "                    'verbose': False}\n",
    "\n",
    "def random_turbulence_models_by_seed(rets,framework_params, seed):\n",
    "  np.random.seed(seed=seed)\n",
    "  randreturns = pd.DataFrame(np.random.randn(rets['AUD'].count(),9), index=rets.index, columns=rets.columns)\n",
    "  roll_turb_signal = rolling_turbulence(randreturns, lookback=260, smoothing=20)\n",
    "  turb_wts = turbulence_weights(roll_turb_signal, framework_params['insample_start_date'], framework_params['insample_end_date'])\n",
    "  model_perf_stats = model_perf(turb_wts, dxy_forward_sq, framework_params)\n",
    "  return model_perf_stats\n",
    "\n",
    "# random_turbulence_models_by_seed(returns,framework_params, 1)\n",
    "\n",
    "rand_turb_perf = pd.DataFrame(list(map(lambda x:random_turbulence_models_by_seed(returns, framework_params, x), range(0,1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_turb_perf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s_yNmqfs6rXx"
   },
   "outputs": [],
   "source": [
    "rand_turb_perf.plot(x='insample excess rmse', y='out-of-sample excess rmse', style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no discernable pattern of performance either in- or out-of-sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_turb_perf.to_pickle('./rand_turb_perf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Part_04b_Creating_Turbulence_Model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:modeliing-volatility] *",
   "language": "python",
   "name": "conda-env-modeliing-volatility-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
